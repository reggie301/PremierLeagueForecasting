{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reggi\\AppData\\Local\\Temp\\ipykernel_3924\\218034486.py:124: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  a['team'] = a['team'].str.replace('.', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "['2017_2018_Fixture', '2018_2019_Fixture', '2019_2020_Fixture', '2020_2021_Fixture', '2021_2022_Fixture', '2022_2023_Fixture']\n",
      "['2017_2018_SPIdata', '2018_2019_SPIdata', '2019_2020_SPIdata', '2020_2021_SPIdata', '2021_2022_SPIdata', '2022_2023_SPIdata']\n",
      "['2017_2018_Fixture', '2018_2019_Fixture', '2019_2020_Fixture', '2020_2021_Fixture', '2021_2022_Fixture', '2022_2023_Fixture']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reggi\\AppData\\Local\\Temp\\ipykernel_3924\\218034486.py:636: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pred['GF_x'] = 0\n",
      "C:\\Users\\reggi\\AppData\\Local\\Temp\\ipykernel_3924\\218034486.py:637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pred['GA_x'] = 0\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "current_season = int(input('What is the current season?'))\n",
    "\n",
    "\n",
    "\n",
    "change_in_table = input('Change in table? (Y/N)')\n",
    "\n",
    "\n",
    "## PREMIER LEAGUE TABLE SCRIPT SCRAPES CURRENT TABLE ##\n",
    "\n",
    "if not os.getcwd().endswith('Football Forecasting Version 2'):\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "current_folder = os.getcwd()\n",
    "\n",
    "relative_folder = fr'{current_season}_{current_season + 1}_Matchweek'\n",
    "\n",
    "full_path = os.path.join(current_folder, relative_folder)\n",
    "\n",
    "isExist = os.path.exists(full_path)\n",
    "\n",
    "if not isExist:\n",
    "    os.makedirs(full_path)\n",
    "\n",
    "matchweek_files = {}\n",
    "\n",
    "for matchweek_file in os.listdir(fr'{current_folder}\\{relative_folder}'):\n",
    "    a = matchweek_file.replace(fr'{current_season}_{current_season+1}_','').replace('.csv','')\n",
    "    matchweek_files[int(a)] = matchweek_file\n",
    "\n",
    "table = pd.read_html(r'https://www.premierleague.com/tables?co=1&se=489&ha=-1')\n",
    "\n",
    "\n",
    "regular_table = pd.DataFrame(table[0])\n",
    "regular_table = regular_table.drop(columns=['Next', 'Unnamed: 12'])\n",
    "regular_table['Match Date'] = regular_table[1::2]['Club'].str.split('-').str[1]\n",
    "regular_table['Match Date'] = regular_table[1::2]['Match Date'].str.split(\n",
    "    ' ').str[2:5]\n",
    "a = []\n",
    "for x in regular_table['Match Date'][1::2]:\n",
    "    try:\n",
    "        a.append(' '.join(x))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "regular_table = regular_table[::2]\n",
    "regular_table['Match Date'] = a\n",
    "regular_table['Match Date'] = pd.to_datetime(regular_table['Match Date'])\n",
    "regular_table.columns = ['Position', 'Club', 'Pl', 'W', 'D',\n",
    "                         'L', 'GF', 'GA', 'GD', 'Pts', 'Form', 'Last Match Date']\n",
    "regular_table['Position'] = regular_table['Position'].str[:3].astype(int)\n",
    "regular_table['Code'] = regular_table['Club'].str[-3:]\n",
    "regular_table['Club'] = regular_table['Club'].str[:-3].str.strip()\n",
    "regular_table['Pl'] = regular_table['Pl'].astype(int)\n",
    "regular_table['W'] = regular_table['W'].astype(int)\n",
    "regular_table['D'] = regular_table['D'].astype(int)\n",
    "regular_table['L'] = regular_table['L'].astype(int)\n",
    "regular_table['GF'] = regular_table['GF'].astype(int)\n",
    "regular_table['GA'] = regular_table['GA'].astype(int)\n",
    "regular_table['GD'] = regular_table['GD'].astype(int)\n",
    "regular_table['Pts'] = regular_table['Pts'].astype(int)\n",
    "regular_table['Form'] = regular_table['Form'].str.replace(\n",
    "    r'\\w{2,100}', '', regex=True)\n",
    "regular_table['Form'] = regular_table['Form'].str.replace(\n",
    "    r\"\\s+\", '', regex=True)\n",
    "regular_table['Form'] = regular_table['Form'].str.replace(\n",
    "    r'[^a-zA-Z]', '', regex=True)\n",
    "regular_table['Form'] = regular_table['Form'].apply(\n",
    "    lambda x: (x.count('W')*3 + x.count('D'))/len(x))\n",
    "regular_table['Season'] = current_season\n",
    "\n",
    "if change_in_table.upper() == 'Y':\n",
    "    regular_table.to_csv(\n",
    "        fr'{full_path}\\{current_season}_{current_season + 1}_{max(matchweek_files)+1}.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SPI SCRAPER FOR THIS MATCHWEEK\n",
    "url = r'https://projects.fivethirtyeight.com/soccer-predictions/premier-league/'\n",
    "\n",
    "\n",
    "date_dict = {'Sept': 'Sep', 'March': 'Mar',\n",
    "             'April': 'Apr', 'June': 'Jun', 'July': 'Jul'}\n",
    "\n",
    "if not os.getcwd().endswith('Football Forecasting Version 2'):\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "current_folder = os.getcwd()\n",
    "\n",
    "relative_folder = fr'{current_season}_{current_season+1}_SPIdata'\n",
    "\n",
    "full_path = os.path.join(current_folder, relative_folder)\n",
    "\n",
    "isExist = os.path.exists(full_path)\n",
    "\n",
    "\n",
    "if not isExist:\n",
    "    os.makedirs(full_path)\n",
    "\n",
    "match_date = date.today().strftime('%y%b%d')\n",
    "\n",
    "dfs = pd.read_html(url)\n",
    "a = dfs[0].replace(' ', ':')\n",
    "a.columns = a.columns.droplevel()\n",
    "a = a[['team', 'spi', 'off.', 'def.']]\n",
    "a['team'] = a['team'].str.replace('\\d+', '', regex=True)\n",
    "a['team'] = a['team'].str.replace('pts', '')\n",
    "a['team'] = a['team'].str.replace('pt', '')\n",
    "a['team'] = a['team'].str.replace('.', '')\n",
    "a['team'] = a['team'].str.strip()\n",
    "a['date'] = match_date\n",
    "a['date'] = pd.to_datetime(a['date'], format='%y%b%d')\n",
    "a = a.rename(columns={'off.': 'off', 'def.': 'def'})\n",
    "\n",
    "a.to_csv(fr'{full_path}\\{current_season}_{current_season+1}_{match_date}.csv')\n",
    "\n",
    "\n",
    "# FIXTURE SCRAPER FROM FBREF\n",
    "\n",
    "# Iterates through premier league seasons\n",
    "for i in range(current_season, current_season + 1):\n",
    "\n",
    "    # Makes sure we are in the right folder\n",
    "\n",
    "    if not os.getcwd().endswith('Football Forecasting Version 2'):\n",
    "        os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "    current_folder = os.getcwd()\n",
    "\n",
    "    relative_folder = fr'{i}_{i+1}_Fixture'\n",
    "\n",
    "    full_path = os.path.join(current_folder, relative_folder)\n",
    "\n",
    "    isExist = os.path.exists(full_path)\n",
    "\n",
    "    # Creates fixture folder if it does not exist\n",
    "\n",
    "    if not isExist:\n",
    "        os.makedirs(full_path)\n",
    "\n",
    "    url = fr'https://fbref.com/en/comps/9/{i}-{i+1}/{i}-{i+1}-Premier-League-Stats'\n",
    "\n",
    "    # Uses user-agent to disguise\n",
    "\n",
    "    HEADERS = {\n",
    "        'User-Agent': 'Mozilla/5.0 (iPhone; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}\n",
    "\n",
    "    # Scrapes fixture data\n",
    "\n",
    "    r = requests.get(url, headers=HEADERS)  # proxies=proxies\n",
    "    print(r)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    List_of_teams = []\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    Team_partial_link = []\n",
    "    Team_full_link = []\n",
    "\n",
    "    table_soup = soup.find(\n",
    "        'table', {'class': 'stats_table sortable min_width force_mobilize'})\n",
    "\n",
    "    for row in table_soup.find_all('tr'):\n",
    "        row_text = [e.text.strip() for e in row.find_all('td')]\n",
    "        try:\n",
    "            List_of_teams.append(row_text[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    row_team = table_soup.find_all(\n",
    "        'td', {'class': 'left', 'data-stat': 'team'})\n",
    "\n",
    "    for e in row_team:\n",
    "        a = e.find('a')\n",
    "        Team_partial_link.append(e.find('a').get('href'))\n",
    "\n",
    "    for e in Team_partial_link:\n",
    "        Team_full_link.append('https://fbref.com' + e)\n",
    "\n",
    "    link_dictionary = dict(zip(Team_full_link, List_of_teams,))\n",
    "\n",
    "    for url in Team_full_link:\n",
    "        time.sleep(3)\n",
    "        # try:\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        table_soup = soup.find(\n",
    "            'table', {'class': 'stats_table sortable min_width', 'id': 'matchlogs_for'})\n",
    "        header_data_fixtures = []\n",
    "        table_data_fixtures = []\n",
    "        date_list = []\n",
    "        new_fixtures_table_data = []\n",
    "\n",
    "        row = table_soup.find('tr')\n",
    "        header_data_fixtures = [e.text.strip() for e in row.find_all('th')]\n",
    "\n",
    "        for row in table_soup.find_all('tr'):\n",
    "            date_text = [e.text.strip()\n",
    "                         for e in row.find_all('th', {'class': 'left'})]\n",
    "            date_list.append(date_text)\n",
    "            row_text = [e.text.strip() for e in row.find_all('td')]\n",
    "            table_data_fixtures.append(row_text)\n",
    "\n",
    "        new_fixture_table_data = []\n",
    "        for x in list(zip(date_list, table_data_fixtures))[1:]:\n",
    "            fixture_row = []\n",
    "            for j in list(x):\n",
    "                for k in j:\n",
    "                    fixture_row.append(k)\n",
    "            new_fixtures_table_data.append(fixture_row)\n",
    "\n",
    "        fixtures_table = pd.DataFrame(\n",
    "            new_fixtures_table_data, columns=header_data_fixtures)\n",
    "        fixtures_table = fixtures_table.replace({'': np.nan})\n",
    "        fixtures_table = fixtures_table[fixtures_table['Comp']\n",
    "                                        == 'Premier League']\n",
    "        fixtures_table = fixtures_table.drop(columns=[\n",
    "                                             'Time', 'Comp', 'Round', 'Day', 'Attendance', 'Captain', 'Formation', 'Referee', 'Match Report', 'Notes'])\n",
    "        fixtures_table['Club'] = link_dictionary[url]\n",
    "        fixtures_table['Date'] = pd.to_datetime(fixtures_table['Date'])\n",
    "\n",
    "        # Saves fixture table as csv\n",
    "        fixtures_table.to_csv(\n",
    "            fr'{full_path}\\{i}_{i+1}_{link_dictionary[url]}.csv')\n",
    "\n",
    "\n",
    "\n",
    "## CREATES MASTER HISTORICAL DATA ##\n",
    "if not os.getcwd().endswith('Football Forecasting Version 2'):\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "\n",
    "# CREATES DICTIONARY OF THREE LETTER ON CLUBS\n",
    "\n",
    "res = []\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a folder\n",
    "    if os.path.isdir(os.path.join(dir_path, path)) and 'Matchweek' in path:\n",
    "        res.append(path)\n",
    "\n",
    "three_let_dict = {}\n",
    "for matchweek_folder in res:\n",
    "    os.chdir(fr'{dir_path}\\{matchweek_folder}')\n",
    "    a = pd.read_csv(random.choice(\n",
    "        os.listdir(fr'{dir_path}\\{matchweek_folder}')))\n",
    "    for index, row in a.iterrows():\n",
    "        three_let_dict[row['Club'].strip()] = row['Code']\n",
    "    os.chdir(dir_path)\n",
    "\n",
    "\n",
    "three_let_dict['Tottenham'] = three_let_dict.pop('Tottenham Hotspur')\n",
    "three_let_dict['Brighton'] = three_let_dict.pop('Brighton and Hove Albion')\n",
    "three_let_dict['Manchester Utd'] = three_let_dict.pop('Manchester United')\n",
    "three_let_dict['Newcastle Utd'] = three_let_dict.pop('Newcastle United')\n",
    "three_let_dict['West Ham'] = three_let_dict.pop('West Ham United')\n",
    "three_let_dict['Wolves'] = three_let_dict.pop('Wolverhampton Wanderers')\n",
    "three_let_dict[\"Nott'ham Forest\"] = three_let_dict.pop('Nottingham Forest')\n",
    "three_let_dict[\"Sheffield Utd\"] = three_let_dict.pop('Sheffield United')\n",
    "three_let_dict[\"Huddersfield\"] = three_let_dict.pop('Huddersfield Town')\n",
    "three_let_dict[\"West Brom\"] = three_let_dict.pop('West Bromwich Albion')\n",
    "\n",
    "\n",
    "# FIXTURE LIST\n",
    "\n",
    "res = []\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a folder\n",
    "    if os.path.isdir(os.path.join(dir_path, path)) and 'Fixture' in path:\n",
    "        res.append(path)\n",
    "print(res)\n",
    "df = pd.DataFrame()\n",
    "for fixtures in res:\n",
    "    os.chdir(fr'{dir_path}\\{fixtures}')\n",
    "    folder = os.getcwd()\n",
    "    list_of_files = os.listdir()\n",
    "    for i in list_of_files:\n",
    "        b = pd.read_csv(fr'{folder}\\{i}', index_col=0)\n",
    "        b['Date'] = pd.to_datetime(b['Date'])\n",
    "        b['Club'] = b['Club'].map(three_let_dict)\n",
    "        b['Opponent'] = b['Opponent'].map(three_let_dict)\n",
    "        df = pd.concat([df, b])\n",
    "    os.chdir(dir_path)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.sort_values(by=['Date'])\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "# SPI DATA\n",
    "\n",
    "res = []\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a folder\n",
    "    if os.path.isdir(os.path.join(dir_path, path)) and 'SPI' in path:\n",
    "        res.append(path)\n",
    "print(res)\n",
    "df1 = pd.DataFrame()\n",
    "for SPIdata in res:\n",
    "    os.chdir(fr'{dir_path}\\{SPIdata}')\n",
    "    folder = os.getcwd()\n",
    "    list_of_files = os.listdir()\n",
    "    for i in list_of_files:\n",
    "        b = pd.read_csv(fr'{folder}\\{i}', index_col=0)\n",
    "        b['date'] = pd.to_datetime(b['date'])\n",
    "        b['Season'] = int(i[:4])\n",
    "        b['team'] = b['team'].str.replace('Leicester', 'Leicester City')\n",
    "        b['team'] = b['team'].str.replace('Newcastle', 'Newcastle Utd')\n",
    "        b['team'] = b['team'].str.replace('Southamon', 'Southampton')\n",
    "        b['team'] = b['team'].str.replace('Norwich', 'Norwich City')\n",
    "        b['team'] = b['team'].str.replace('Man City', 'Manchester City')\n",
    "        b['team'] = b['team'].str.replace('Man United', 'Manchester Utd')\n",
    "        b['team'] = b['team'].str.replace('Nottm Forest', \"Nott'ham Forest\")\n",
    "        b['team'] = b['team'].map(three_let_dict)\n",
    "        df1 = pd.concat([df1, b])\n",
    "    os.chdir(dir_path)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1 = df1.sort_values(by=['date'])\n",
    "\n",
    "\n",
    "# PREMIER LEAGUE TABLE\n",
    "res = []\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a folder\n",
    "    if os.path.isdir(os.path.join(dir_path, path)) and 'Matchweek' in path:\n",
    "        res.append(path)\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "for matchweek in res:\n",
    "    os.chdir(fr'{dir_path}\\{matchweek}')\n",
    "    folder = os.getcwd()\n",
    "    list_of_files = os.listdir()\n",
    "    for i in list_of_files:\n",
    "        a = pd.read_csv(fr'{folder}\\{i}', index_col=0)\n",
    "        a['Season'] = int(matchweek[:4])\n",
    "        a['Last Match Date'] = pd.to_datetime(a['Last Match Date'])\n",
    "        a = a.drop(columns=['Club', 'W', 'D', 'L', 'GF', 'GA', 'GD'])\n",
    "        df2 = pd.concat([df2, a])\n",
    "    os.chdir(dir_path)\n",
    "\n",
    "df2 = df2.sort_values(by=['Last Match Date'])\n",
    "\n",
    "\n",
    "fixture_fixture_merge = pd.merge_asof(left=df, right=df, left_on=['Date'], right_on=[\n",
    "                                      'Date'], left_by=['Club'], right_by=['Opponent'], direction='nearest')\n",
    "fixture_fixture_merge = fixture_fixture_merge.drop(\n",
    "    columns=['Venue_y', 'Result_y', 'Opponent_y', 'Club_y'])\n",
    "matchweek_fixture_merge_1 = pd.merge_asof(left=fixture_fixture_merge, right=df2, left_on=[\n",
    "                                          'Date'], right_on=['Last Match Date'], left_by=['Club_x'], right_by=['Code'], direction='nearest')\n",
    "matchweek_fixture_merge_1 = matchweek_fixture_merge_1.drop(\n",
    "    columns=['Last Match Date', 'Code'])\n",
    "matchweek_fixture_merge_2 = pd.merge_asof(left=matchweek_fixture_merge_1, right=df2, left_on=[\n",
    "                                          'Date'], right_on=['Last Match Date'], left_by=['Opponent_x'], right_by=['Code'], direction='nearest')\n",
    "matchweek_fixture_merge_2 = matchweek_fixture_merge_2.drop(\n",
    "    columns=['Season_x', 'Last Match Date', 'Code'])\n",
    "matchweek_fixture_merge_1 = pd.merge_asof(left=fixture_fixture_merge, right=df2, left_on=[\n",
    "                                          'Date'], right_on=['Last Match Date'], left_by=['Club_x'], right_by=['Code'], direction='nearest')\n",
    "matchweek_fixture_merge_1 = matchweek_fixture_merge_1.drop(\n",
    "    columns=['Last Match Date', 'Code'])\n",
    "matchweek_fixture_merge_2 = pd.merge_asof(left=matchweek_fixture_merge_1, right=df2, left_on=[\n",
    "                                          'Date'], right_on=['Last Match Date'], left_by=['Opponent_x'], right_by=['Code'], direction='nearest')\n",
    "matchweek_fixture_merge_2 = matchweek_fixture_merge_2.drop(\n",
    "    columns=['Season_x', 'Last Match Date', 'Code'])\n",
    "rename_columns = {'Venue_x': 'Venue', 'Result_x': 'Result',\n",
    "                  'Club_x': 'Club', 'Season_x': 'Season', 'Opponent_x': 'Opp'}\n",
    "spi_everything_merge_1 = pd.merge_asof(left=matchweek_fixture_merge_2, right=df1, left_on=[\n",
    "                                       'Date'], right_on=['date'], left_by=['Club_x'], right_by=['team'], direction='nearest')\n",
    "spi_everything_merge_1 = spi_everything_merge_1.drop(\n",
    "    columns=['team', 'date', 'Season_y'])\n",
    "spi_everything_merge_2 = pd.merge_asof(left=spi_everything_merge_1, right=df1, left_on=[\n",
    "                                       'Date'], right_on=['date'], left_by=['Opponent_x'], right_by=['team'], direction='nearest')\n",
    "spi_everything_merge_2 = spi_everything_merge_2.drop(\n",
    "    columns=['team', 'date', 'Season_y', 'GF_y', 'GA_y', 'xG_y', 'xGA_y'])\n",
    "df_final = spi_everything_merge_2.rename(rename_columns, axis='columns')\n",
    "\n",
    "df_final = df_final.sort_values(by=['Club', 'Season', 'Date'])\n",
    "df_final['Pl_x'] = df_final.groupby(['Club', 'Season'])['Pl_x'].shift(1)\n",
    "df_final['Position_x'] = df_final.groupby(['Club', 'Season'])[\n",
    "    'Position_x'].shift(1)\n",
    "df_final['Pts_x'] = df_final.groupby(['Club', 'Season'])['Pts_x'].shift(1)\n",
    "df_final['Form_x'] = df_final.groupby(['Club', 'Season'])['Pts_x'].shift(1)\n",
    "\n",
    "\n",
    "df_final = df_final.sort_values(by=['Opp', 'Season', 'Date'])\n",
    "df_final['Pl_y'] = df_final.groupby(['Opp', 'Season'])['Pl_y'].shift(1)\n",
    "df_final['Position_y'] = df_final.groupby(\n",
    "    ['Opp', 'Season'])['Position_y'].shift(1)\n",
    "df_final['Pts_y'] = df_final.groupby(['Opp', 'Season'])['Pts_y'].shift(1)\n",
    "df_final['Form_y'] = df_final.groupby(['Opp', 'Season'])['Pts_y'].shift(1)\n",
    "\n",
    "df_final = df_final.sort_values(by=['Club', 'Season', 'Date'])\n",
    "\n",
    "# df_final = df_final.dropna()\n",
    "\n",
    "if not os.getcwd().endswith('Football Forecasting Version 2'):\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "os.chdir(fr'{dir_path}\\{current_season}_{current_season + 1}_Clean_Data')\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "df_final.to_csv(\n",
    "    fr'{current_season}_{current_season + 1}_{today.strftime(\"%b%d\")}_historical_data.csv')\n",
    "\n",
    "## PREPARES PREDICTION TABLE ###\n",
    "\n",
    "if not os.getcwd().endswith('Football Forecasting Version 2'):\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "# FIXTURE LIST\n",
    "\n",
    "res = []\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a folder\n",
    "    if os.path.isdir(os.path.join(dir_path, path)) and 'Fixture' in path:\n",
    "        res.append(path)\n",
    "print(res)\n",
    "df = pd.DataFrame()\n",
    "for fixtures in res:\n",
    "    os.chdir(fr'{dir_path}\\{fixtures}')\n",
    "    folder = os.getcwd()\n",
    "    list_of_files = os.listdir()\n",
    "    for i in list_of_files:\n",
    "        b = pd.read_csv(fr'{folder}\\{i}', index_col=0)\n",
    "        b['Date'] = pd.to_datetime(b['Date'])\n",
    "        b['Club'] = b['Club'].map(three_let_dict)\n",
    "        b['Opponent'] = b['Opponent'].map(three_let_dict)\n",
    "        b = b[b['Date'] >= np.datetime64('today')].iloc[:1]\n",
    "        df = pd.concat([df, b])\n",
    "    os.chdir(dir_path)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.sort_values(by=['Date'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "fixture_fixture_merge = pd.merge_asof(left=df, right=df, left_on=['Date'], right_on=[\n",
    "                                      'Date'], left_by=['Club'], right_by=['Opponent'], direction='nearest')\n",
    "fixture_fixture_merge = fixture_fixture_merge.drop(\n",
    "    columns=['Venue_y', 'Result_y', 'Opponent_y', 'Club_y'])\n",
    "matchweek_fixture_merge_1 = pd.merge_asof(left=fixture_fixture_merge, right=df2, left_on=[\n",
    "                                          'Date'], right_on=['Last Match Date'], left_by=['Club_x'], right_by=['Code'], direction='nearest')\n",
    "matchweek_fixture_merge_1 = matchweek_fixture_merge_1.drop(\n",
    "    columns=['Last Match Date', 'Code'])\n",
    "matchweek_fixture_merge_2 = pd.merge_asof(left=matchweek_fixture_merge_1, right=df2, left_on=[\n",
    "                                          'Date'], right_on=['Last Match Date'], left_by=['Opponent_x'], right_by=['Code'], direction='nearest')\n",
    "matchweek_fixture_merge_2 = matchweek_fixture_merge_2.drop(\n",
    "    columns=['Season_x', 'Last Match Date', 'Code'])\n",
    "spi_everything_merge_1 = pd.merge_asof(left=matchweek_fixture_merge_2, right=df1, left_on=[\n",
    "                                       'Date'], right_on=['date'], left_by=['Club_x'], right_by=['team'], direction='nearest')\n",
    "spi_everything_merge_1 = spi_everything_merge_1.drop(\n",
    "    columns=['team', 'date', 'Season_y'])\n",
    "spi_everything_merge_2 = pd.merge_asof(left=spi_everything_merge_1, right=df1, left_on=[\n",
    "                                       'Date'], right_on=['date'], left_by=['Opponent_x'], right_by=['team'], direction='nearest')\n",
    "spi_everything_merge_2 = spi_everything_merge_2.drop(\n",
    "    columns=['team', 'date', 'Season_y', 'GF_y', 'GA_y', 'xG_y', 'xGA_y'])\n",
    "\n",
    "rename_columns = {'Venue_x': 'Venue', 'Result_x': 'Result',\n",
    "                  'Club_x': 'Club', 'Season_x': 'Season', 'Opponent_x': 'Opp'}\n",
    "\n",
    "df_final = spi_everything_merge_2.rename(rename_columns, axis='columns')\n",
    "\n",
    "\n",
    "if not os.getcwd().endswith('Football Forecasting Version 2'):\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "os.chdir(fr'{dir_path}\\{current_season}_{current_season + 1}_Clean_Data')\n",
    "\n",
    "df_final.to_csv(\n",
    "    fr'{current_season}_{current_season + 1}_{today.strftime(\"%b%d\")}_matchweek_data.csv')\n",
    "\n",
    "### REGRESSION PREDICTOR ###\n",
    "\n",
    "if not os.getcwd().endswith('Football Forecasting Version 2'):\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "df_train = pd.read_csv(\n",
    "    fr'{dir_path}\\{current_season}_{current_season + 1}_Clean_Data\\{current_season}_{current_season + 1}_{today.strftime(\"%b%d\")}_historical_data.csv', index_col=0)\n",
    "\n",
    "df_pred = pd.read_csv(\n",
    "    fr'{dir_path}\\{current_season}_{current_season + 1}_Clean_Data\\{current_season}_{current_season + 1}_{today.strftime(\"%b%d\")}_matchweek_data.csv', index_col=0)\n",
    "\n",
    "\n",
    "df = pd.concat([df_train, df_pred])\n",
    "df = df.sort_values(by=['Club', 'Season', 'Date',])\n",
    "df = df.reset_index(drop=True)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "## RESULT ENCODING ##\n",
    "\n",
    "\n",
    "def encode_result(x):\n",
    "    if x['Result'] == 'W':\n",
    "        val = 2\n",
    "    elif x['Result'] == 'D':\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "\n",
    "df['Result'] = df.apply(encode_result, axis=1)\n",
    "\n",
    "# POINTS DIFF#\n",
    "df['Points_Diff'] = (df['Pts_x']/df['Pl_x'] - df['Pts_y']/df['Pl_y'])/3\n",
    "\n",
    "\n",
    "## SPI Diff##\n",
    "df['SPI_Diff'] = df['spi_x'] - df['spi_y']\n",
    "df['SPI_Diff'] = (df['SPI_Diff'] - df.groupby(['Season'])['SPI_Diff'].transform(min))/(df.groupby(\n",
    "    ['Season'])['SPI_Diff'].transform(max) - df.groupby(['Season'])['SPI_Diff'].transform(min))\n",
    "df['Off_Diff'] = df['off_x'] - df['off_y']\n",
    "df['Off_Diff'] = (df['Off_Diff'] - df.groupby(['Season'])['Off_Diff'].transform(min))/(df.groupby(\n",
    "    ['Season'])['Off_Diff'].transform(max) - df.groupby(['Season'])['Off_Diff'].transform(min))\n",
    "df['Def_Diff'] = df['def_x'] - df['def_y']\n",
    "df['Def_Diff'] = (df['Def_Diff'] - df.groupby(['Season'])['Def_Diff'].transform(min))/(df.groupby(\n",
    "    ['Season'])['Def_Diff'].transform(max) - df.groupby(['Season'])['Def_Diff'].transform(min))\n",
    "\n",
    "## FORM##\n",
    "df['Form_Diff'] = (df['Form_x'] - df['Form_y'])/15\n",
    "\n",
    "# Creating form for goals and goals conceded and expected values for last 5 games\n",
    "df['Avg_GF_last_5'] = df.groupby(['Club', 'Season'])['GF_x'].shift(1)\n",
    "df['Avg_GF_last_5'] = df.groupby(['Club', 'Season'])['Avg_GF_last_5'].rolling(\n",
    "    5).mean().reset_index([0, 1], drop=True)\n",
    "df['Avg_GA_last_5'] = df.groupby(['Club', 'Season'])['GA_x'].shift(1)\n",
    "df['Avg_GA_last_5'] = df.groupby(['Club', 'Season'])['Avg_GA_last_5'].rolling(\n",
    "    5).mean().reset_index([0, 1], drop=True)\n",
    "df['Avg_xG_last_5'] = df.groupby(['Club', 'Season'])['xG_x'].shift(1)\n",
    "df['Avg_xG_last_5'] = df.groupby(['Club', 'Season'])['Avg_xG_last_5'].rolling(\n",
    "    5).mean().reset_index([0, 1], drop=True)\n",
    "df['Avg_xGA_last_5'] = df.groupby(['Club', 'Season'])['xGA_x'].shift(1)\n",
    "df['Avg_xGA_last_5'] = df.groupby(['Club', 'Season'])['Avg_xGA_last_5'].rolling(\n",
    "    5).mean().reset_index([0, 1], drop=True)\n",
    "df['Avg_Poss_last_5'] = df.groupby(['Club', 'Season'])['Poss_x'].shift(1)\n",
    "df['Avg_Poss_last_5'] = df.groupby(['Club', 'Season'])[\n",
    "    'Avg_Poss_last_5'].rolling(5).mean().reset_index([0, 1], drop=True)\n",
    "\n",
    "# Standardisation\n",
    "df['Avg_GF_last_5'] = (df['Avg_GF_last_5'] - df.groupby(['Season'])['Avg_GF_last_5'].transform(min))/(\n",
    "    df.groupby(['Season'])['Avg_GF_last_5'].transform(max) - df.groupby(['Season'])['Avg_GF_last_5'].transform(min))\n",
    "df['Avg_GA_last_5'] = (df['Avg_GA_last_5'] - df.groupby(['Season'])['Avg_GA_last_5'].transform(min))/(\n",
    "    df.groupby(['Season'])['Avg_GA_last_5'].transform(max) - df.groupby(['Season'])['Avg_GA_last_5'].transform(min))\n",
    "df['Avg_xG_last_5'] = (df['Avg_xG_last_5'] - df.groupby(['Season'])['Avg_xG_last_5'].transform(min))/(\n",
    "    df.groupby(['Season'])['Avg_xG_last_5'].transform(max) - df.groupby(['Season'])['Avg_xG_last_5'].transform(min))\n",
    "df['Avg_xGA_last_5'] = (df['Avg_xGA_last_5'] - df.groupby(['Season'])['Avg_xGA_last_5'].transform(min))/(\n",
    "    df.groupby(['Season'])['Avg_xGA_last_5'].transform(max) - df.groupby(['Season'])['Avg_xGA_last_5'].transform(min))\n",
    "df['Avg_Poss_last_5'] = (df['Avg_Poss_last_5'] - df.groupby(['Season'])['Avg_Poss_last_5'].transform(min))/(\n",
    "    df.groupby(['Season'])['Avg_Poss_last_5'].transform(max) - df.groupby(['Season'])['Avg_Poss_last_5'].transform(min))\n",
    "\n",
    "## SEASON STATS##\n",
    "df['Avg_GF_season'] = df.groupby(['Club', 'Season'])['GF_x'].shift(1)\n",
    "df['Avg_GF_season'] = df.groupby(['Season'])['Avg_GF_season'].expanding(\n",
    "    1).mean().reset_index([0], drop=True)\n",
    "df['Avg_GA_season'] = df.groupby(['Club', 'Season'])['GA_x'].shift(1)\n",
    "df['Avg_GA_season'] = df.groupby(['Season'])['Avg_GA_season'].expanding(\n",
    "    1).mean().reset_index([0], drop=True)\n",
    "df['Avg_xG_season'] = df.groupby(['Club', 'Season'])['xG_x'].shift(1)\n",
    "df['Avg_xG_season'] = df.groupby(['Season'])['Avg_xG_season'].expanding(\n",
    "    1).mean().reset_index([0], drop=True)\n",
    "df['Avg_xGA_season'] = df.groupby(['Club', 'Season'])['xGA_x'].shift(1)\n",
    "df['Avg_xGA_season'] = df.groupby(['Season'])['Avg_xGA_season'].expanding(\n",
    "    1).mean().reset_index([0], drop=True)\n",
    "df['Avg_Poss_season'] = df.groupby(['Club', 'Season'])['Poss_x'].shift(1)\n",
    "df['Avg_Poss_season'] = df.groupby(['Season'])['Avg_Poss_season'].expanding(\n",
    "    1).mean().reset_index([0], drop=True)\n",
    "\n",
    "# Standardisation\n",
    "df['Avg_GF_season'] = (df['Avg_GF_season'] - df.groupby(['Season'])['Avg_GF_season'].transform(min))/(\n",
    "    df.groupby(['Season'])['Avg_GF_season'].transform(max) - df.groupby(['Season'])['Avg_GF_season'].transform(min))\n",
    "df['Avg_GA_season'] = (df['Avg_GA_season'] - df.groupby(['Season'])['Avg_GA_season'].transform(min))/(\n",
    "    df.groupby(['Season'])['Avg_GA_season'].transform(max) - df.groupby(['Season'])['Avg_GA_season'].transform(min))\n",
    "df['Avg_xG_season'] = (df['Avg_xG_season'] - df.groupby(['Season'])['Avg_xG_season'].transform(min))/(\n",
    "    df.groupby(['Season'])['Avg_xG_season'].transform(max) - df.groupby(['Season'])['Avg_xG_season'].transform(min))\n",
    "df['Avg_xGA_season'] = (df['Avg_xGA_season'] - df.groupby(['Season'])['Avg_xGA_season'].transform(min))/(\n",
    "    df.groupby(['Season'])['Avg_xGA_season'].transform(max) - df.groupby(['Season'])['Avg_xGA_season'].transform(min))\n",
    "df['Avg_Poss_season'] = (df['Avg_Poss_season'] - df.groupby(['Season'])['Avg_Poss_season'].transform(min))/(\n",
    "    df.groupby(['Season'])['Avg_Poss_season'].transform(max) - df.groupby(['Season'])['Avg_Poss_season'].transform(min))\n",
    "\n",
    "## AGAINST OPPONENT##\n",
    "df['Avg_GF_Opp'] = df.groupby(['Club', 'Opp'])['GF_x'].shift(1)\n",
    "df['Avg_GF_Opp'] = df.groupby(['Club', 'Opp'])['Avg_GF_Opp'].rolling(\n",
    "    2).mean().reset_index([0, 1], drop=True)\n",
    "df['Avg_GA_Opp'] = df.groupby(['Club', 'Opp'])['GA_x'].shift(1)\n",
    "df['Avg_GA_Opp'] = df.groupby(['Club', 'Opp'])['Avg_GA_Opp'].rolling(\n",
    "    2).mean().reset_index([0, 1], drop=True)\n",
    "df['Avg_xG_Opp'] = df.groupby(['Club', 'Opp'])['xG_x'].shift(1)\n",
    "df['Avg_xG_Opp'] = df.groupby(['Club', 'Opp'])['Avg_xG_Opp'].rolling(\n",
    "    2).mean().reset_index([0, 1], drop=True)\n",
    "df['Avg_xGA_Opp'] = df.groupby(['Club', 'Opp'])['xGA_x'].shift(1)\n",
    "df['Avg_xGA_Opp'] = df.groupby(['Club', 'Opp'])['Avg_xGA_Opp'].rolling(\n",
    "    2).mean().reset_index([0, 1], drop=True)\n",
    "df['Avg_Poss_Opp'] = df.groupby(['Club', 'Opp'])['Poss_x'].shift(1)\n",
    "df['Avg_Poss_Opp'] = df.groupby(['Club', 'Opp'])['Avg_Poss_Opp'].rolling(\n",
    "    2).mean().reset_index([0, 1], drop=True)\n",
    "\n",
    "# Standardisation\n",
    "df['Avg_GF_Opp'] = (df['Avg_GF_Opp'] - df.groupby(['Opp'])['Avg_GF_Opp'].transform(min))/(\n",
    "    df.groupby(['Opp'])['Avg_GF_Opp'].transform(max) - df.groupby(['Opp'])['Avg_GF_Opp'].transform(min))\n",
    "df['Avg_GA_Opp'] = (df['Avg_GA_Opp'] - df.groupby(['Opp'])['Avg_GA_Opp'].transform(min))/(\n",
    "    df.groupby(['Opp'])['Avg_GA_Opp'].transform(max) - df.groupby(['Opp'])['Avg_GA_Opp'].transform(min))\n",
    "df['Avg_xG_Opp'] = (df['Avg_xG_Opp'] - df.groupby(['Opp'])['Avg_xG_Opp'].transform(min))/(\n",
    "    df.groupby(['Opp'])['Avg_xG_Opp'].transform(max) - df.groupby(['Opp'])['Avg_xG_Opp'].transform(min))\n",
    "df['Avg_xGA_Opp'] = (df['Avg_xGA_Opp'] - df.groupby(['Opp'])['Avg_xGA_Opp'].transform(min))/(\n",
    "    df.groupby(['Opp'])['Avg_xGA_Opp'].transform(max) - df.groupby(['Opp'])['Avg_xGA_Opp'].transform(min))\n",
    "df['Avg_Poss_Opp'] = (df['Avg_Poss_Opp'] - df.groupby(['Opp'])['Avg_Poss_Opp'].transform(min))/(\n",
    "    df.groupby(['Opp'])['Avg_Poss_Opp'].transform(max) - df.groupby(['Opp'])['Avg_Poss_Opp'].transform(min))\n",
    "\n",
    "\n",
    "df_test = df[df['Date'] < np.datetime64('today')]\n",
    "df_pred = df[df['Date'] >= np.datetime64('today')]\n",
    "df_pred['GF_x'] = 0\n",
    "df_pred['GA_x'] = 0\n",
    "df_poo = df_pred\n",
    "\n",
    "\n",
    "unwanted_columns = ['Date', 'Opp', 'Result', 'GF_x', 'xG_x', 'xGA_x',\n",
    "                    'Poss_x', 'Club', 'Poss_y', 'Position_x', 'Pl_x', 'Pts_x', 'Form_x',\n",
    "                    'Position_y', 'Pl_y', 'Pts_y', 'Form_y', 'spi_x', 'off_x', 'def_x',\n",
    "                    'Season', 'spi_y', 'off_y', 'def_y']\n",
    "df_test_1 = df_test.drop(columns=unwanted_columns)\n",
    "df_pred_1 = df_pred.drop(columns=unwanted_columns)\n",
    "df_test_1 = df_test_1.dropna()\n",
    "df_pred_1 = df_pred_1.dropna()\n",
    "\n",
    "df_test_1 = pd.get_dummies(df_test_1, columns=['Venue'])\n",
    "df_pred_1 = pd.get_dummies(df_pred_1, columns=['Venue'])\n",
    "\n",
    "boo = df_test_1.drop(columns='GA_x')\n",
    "\n",
    "X_train_1 = df_test_1[boo.columns]\n",
    "y_train_1 = df_test_1['GA_x']\n",
    "\n",
    "X_test_1 = df_pred_1[boo.columns]\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=250, max_depth=3, random_state=0)\n",
    "rfr.fit(X_train_1, y_train_1)\n",
    "y_pred_1 = rfr.predict(X_test_1)\n",
    "\n",
    "\n",
    "unwanted_columns = ['Date', 'Opp', 'Result', 'GA_x', 'xG_x', 'xGA_x',\n",
    "                    'Poss_x', 'Club', 'Poss_y', 'Position_x', 'Pl_x', 'Pts_x', 'Form_x',\n",
    "                    'Position_y', 'Pl_y', 'Pts_y', 'Form_y', 'spi_x', 'off_x', 'def_x',\n",
    "                    'Season', 'spi_y', 'off_y', 'def_y']\n",
    "df_test = df_test.drop(columns=unwanted_columns)\n",
    "df_pred = df_pred.drop(columns=unwanted_columns)\n",
    "df_test = df_test.dropna()\n",
    "df_pred = df_pred.dropna()\n",
    "\n",
    "df_test.columns\n",
    "df_test = pd.get_dummies(df_test, columns=['Venue'])\n",
    "df_pred = pd.get_dummies(df_pred, columns=['Venue'])\n",
    "\n",
    "boo = df_test.drop(columns='GF_x')\n",
    "\n",
    "X_train = df_test[boo.columns]\n",
    "y_train = df_test['GF_x']\n",
    "\n",
    "X_test = df_pred[boo.columns]\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=90, max_depth=3, random_state=0)\n",
    "rfr.fit(X_train, y_train)\n",
    "y_pred = rfr.predict(X_test)\n",
    "\n",
    "df_poo = df_poo[['Season', 'Club', 'Opp', 'Venue', 'Date']]\n",
    "a = pd.DataFrame(y_pred, columns=['GF'], index=X_test.index)\n",
    "b = pd.DataFrame(y_pred_1, columns=['GA'], index=X_test_1.index)\n",
    "c = pd.merge(df_poo, a, left_index=True, right_index=True)\n",
    "d1 = pd.merge(c, b, left_index=True, right_index=True)\n",
    "d2 = pd.merge(d1, d1, left_on=['Club', 'Date'], right_on=['Opp', 'Date'])\n",
    "d2 = d2.drop(columns=['Season_y', 'Club_y', 'Opp_y', 'Venue_y'])\n",
    "\n",
    "d2['GF'] = (d2['GF_x'] + d2['GA_y']) / 2\n",
    "d2['GA'] = (d2['GF_y'] + d2['GA_x']) / 2\n",
    "d2 = d2.drop(columns=['GF_x', 'GA_x', 'GF_y', 'GA_y'])\n",
    "\n",
    "d3 = d2.drop(columns=['Venue_x', 'GA'])\n",
    "\n",
    "\n",
    "for score in range(6):\n",
    "    d3['Score' + str(score)] = (d3['GF'] ** score *\n",
    "                                np.exp(- d3['GF']))/np.math.factorial(score)\n",
    "\n",
    "d4 = pd.merge(d3, d3, left_on='Club_x', right_on='Opp_x')\n",
    "\n",
    "score_list = []\n",
    "\n",
    "win_list = []\n",
    "draw_list = []\n",
    "loss_list = []\n",
    "\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        d4[fr'{i} - {j}'] = d4[fr'Score{i}_x'] * d4[fr'Score{j}_y']\n",
    "        score_list.append(fr'{i} - {j}')\n",
    "\n",
    "        if i > j:\n",
    "            win_list.append(list(d4[fr'Score{i}_x'] * d4[fr'Score{j}_y']))\n",
    "        elif i == j:\n",
    "            draw_list.append(list(d4[fr'Score{i}_x'] * d4[fr'Score{j}_y']))\n",
    "        else:\n",
    "            loss_list.append(list(d4[fr'Score{i}_x'] * d4[fr'Score{j}_y']))\n",
    "\n",
    "\n",
    "d4[score_list].idxmax(axis=1)\n",
    "\n",
    "d2['Most likely score'] = list(d4[score_list].idxmax(axis=1))\n",
    "\n",
    "win_list = [sum(i) for i in list(zip(*win_list))]\n",
    "draw_list = [sum(i) for i in list(zip(*draw_list))]\n",
    "loss_list = [sum(i) for i in list(zip(*loss_list))]\n",
    "\n",
    "d2['win_prob'] = win_list\n",
    "d2['draw_prob'] = draw_list\n",
    "d2['loss_prob'] = loss_list\n",
    "\n",
    "d2['sum'] = d2['win_prob'] + d2['draw_prob'] + d2['loss_prob']\n",
    "\n",
    "d2['win_prob'] = d2['win_prob']/d2['sum'] \n",
    "d2['draw_prob'] = d2['draw_prob']/d2['sum'] \n",
    "d2['loss_prob'] = d2['loss_prob']/d2['sum'] \n",
    "\n",
    "d2 = d2.drop(columns = ['sum'])\n",
    "\n",
    "if not os.getcwd().endswith('Football Forecasting Version 2'):\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "current_folder = os.getcwd()\n",
    "\n",
    "relative_folder = fr'{current_season}_{current_season + 1}_Match_Predictions'\n",
    "\n",
    "full_path = os.path.join(current_folder, relative_folder)\n",
    "\n",
    "# d2.to_csv(\n",
    "#     fr'{full_path}\\{current_season}_{current_season + 1}_{today.strftime(\"%b%d\")}_predictions.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d2 = d2[d2['Venue_x'] == 'Home']\n",
    "d2 = d2.drop(columns = ['Season_x','GF','GA','Venue_x'])\n",
    "d2['Date'] = d2['Date'].dt.strftime('%b%d')\n",
    "d2 = d2.rename(columns={\"Club_x\": \"Home\", \"Opp_x\": \"Away\",\"win_prob\": \"1\", \"draw_prob\":\"X\",\"loss_prob\":\"2\"})\n",
    "d2 = d2.reset_index(drop=True)\n",
    "final = d2.style.background_gradient(axis=None).format({'1': '{:,.2%}'.format,'X': '{:,.2%}'.format,'2': '{:,.2%}'.format,})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_63846_row0_col4 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_63846_row0_col5 {\n",
       "  background-color: #ede7f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row0_col6 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row1_col4 {\n",
       "  background-color: #348ebf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_63846_row1_col5 {\n",
       "  background-color: #d2d3e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row1_col6 {\n",
       "  background-color: #d0d1e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row2_col4 {\n",
       "  background-color: #dddbec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row2_col5 {\n",
       "  background-color: #d6d6e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row2_col6 {\n",
       "  background-color: #167bb6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_63846_row3_col4 {\n",
       "  background-color: #3f93c2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_63846_row3_col5 {\n",
       "  background-color: #d1d2e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row3_col6 {\n",
       "  background-color: #cacee5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row4_col4 {\n",
       "  background-color: #a1bbda;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row4_col5, #T_63846_row7_col5 {\n",
       "  background-color: #ced0e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row4_col6 {\n",
       "  background-color: #7eadd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_63846_row5_col4 {\n",
       "  background-color: #f0eaf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row5_col5 {\n",
       "  background-color: #e2dfee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row5_col6 {\n",
       "  background-color: #045e94;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_63846_row6_col4 {\n",
       "  background-color: #0f76b3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_63846_row6_col5 {\n",
       "  background-color: #d8d7e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row6_col6 {\n",
       "  background-color: #e0dded;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row7_col4 {\n",
       "  background-color: #91b5d6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row7_col6 {\n",
       "  background-color: #8eb3d5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row8_col4 {\n",
       "  background-color: #187cb6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_63846_row8_col5 {\n",
       "  background-color: #d7d6e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_63846_row8_col6 {\n",
       "  background-color: #dbdaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_63846\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_63846_level0_col0\" class=\"col_heading level0 col0\" >Home</th>\n",
       "      <th id=\"T_63846_level0_col1\" class=\"col_heading level0 col1\" >Away</th>\n",
       "      <th id=\"T_63846_level0_col2\" class=\"col_heading level0 col2\" >Date</th>\n",
       "      <th id=\"T_63846_level0_col3\" class=\"col_heading level0 col3\" >Most likely score</th>\n",
       "      <th id=\"T_63846_level0_col4\" class=\"col_heading level0 col4\" >1</th>\n",
       "      <th id=\"T_63846_level0_col5\" class=\"col_heading level0 col5\" >X</th>\n",
       "      <th id=\"T_63846_level0_col6\" class=\"col_heading level0 col6\" >2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_63846_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_63846_row0_col0\" class=\"data row0 col0\" >ARS</td>\n",
       "      <td id=\"T_63846_row0_col1\" class=\"data row0 col1\" >LEE</td>\n",
       "      <td id=\"T_63846_row0_col2\" class=\"data row0 col2\" >Apr01</td>\n",
       "      <td id=\"T_63846_row0_col3\" class=\"data row0 col3\" >2 - 0</td>\n",
       "      <td id=\"T_63846_row0_col4\" class=\"data row0 col4\" >67.89%</td>\n",
       "      <td id=\"T_63846_row0_col5\" class=\"data row0 col5\" >19.43%</td>\n",
       "      <td id=\"T_63846_row0_col6\" class=\"data row0 col6\" >12.68%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63846_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_63846_row1_col0\" class=\"data row1 col0\" >BHA</td>\n",
       "      <td id=\"T_63846_row1_col1\" class=\"data row1 col1\" >BRE</td>\n",
       "      <td id=\"T_63846_row1_col2\" class=\"data row1 col2\" >Apr01</td>\n",
       "      <td id=\"T_63846_row1_col3\" class=\"data row1 col3\" >1 - 1</td>\n",
       "      <td id=\"T_63846_row1_col4\" class=\"data row1 col4\" >47.49%</td>\n",
       "      <td id=\"T_63846_row1_col5\" class=\"data row1 col5\" >25.99%</td>\n",
       "      <td id=\"T_63846_row1_col6\" class=\"data row1 col6\" >26.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63846_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_63846_row2_col0\" class=\"data row2 col0\" >BOU</td>\n",
       "      <td id=\"T_63846_row2_col1\" class=\"data row2 col1\" >FUL</td>\n",
       "      <td id=\"T_63846_row2_col2\" class=\"data row2 col2\" >Apr01</td>\n",
       "      <td id=\"T_63846_row2_col3\" class=\"data row2 col3\" >0 - 1</td>\n",
       "      <td id=\"T_63846_row2_col4\" class=\"data row2 col4\" >23.36%</td>\n",
       "      <td id=\"T_63846_row2_col5\" class=\"data row2 col5\" >24.99%</td>\n",
       "      <td id=\"T_63846_row2_col6\" class=\"data row2 col6\" >51.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63846_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_63846_row3_col0\" class=\"data row3 col0\" >CHE</td>\n",
       "      <td id=\"T_63846_row3_col1\" class=\"data row3 col1\" >AVL</td>\n",
       "      <td id=\"T_63846_row3_col2\" class=\"data row3 col2\" >Apr01</td>\n",
       "      <td id=\"T_63846_row3_col3\" class=\"data row3 col3\" >1 - 1</td>\n",
       "      <td id=\"T_63846_row3_col4\" class=\"data row3 col4\" >46.31%</td>\n",
       "      <td id=\"T_63846_row3_col5\" class=\"data row3 col5\" >26.27%</td>\n",
       "      <td id=\"T_63846_row3_col6\" class=\"data row3 col6\" >27.43%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63846_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_63846_row4_col0\" class=\"data row4 col0\" >CRY</td>\n",
       "      <td id=\"T_63846_row4_col1\" class=\"data row4 col1\" >LEI</td>\n",
       "      <td id=\"T_63846_row4_col2\" class=\"data row4 col2\" >Apr01</td>\n",
       "      <td id=\"T_63846_row4_col3\" class=\"data row4 col3\" >1 - 1</td>\n",
       "      <td id=\"T_63846_row4_col4\" class=\"data row4 col4\" >34.22%</td>\n",
       "      <td id=\"T_63846_row4_col5\" class=\"data row4 col5\" >26.81%</td>\n",
       "      <td id=\"T_63846_row4_col6\" class=\"data row4 col6\" >38.98%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63846_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_63846_row5_col0\" class=\"data row5 col0\" >EVE</td>\n",
       "      <td id=\"T_63846_row5_col1\" class=\"data row5 col1\" >TOT</td>\n",
       "      <td id=\"T_63846_row5_col2\" class=\"data row5 col2\" >Apr03</td>\n",
       "      <td id=\"T_63846_row5_col3\" class=\"data row5 col3\" >0 - 1</td>\n",
       "      <td id=\"T_63846_row5_col4\" class=\"data row5 col4\" >18.45%</td>\n",
       "      <td id=\"T_63846_row5_col5\" class=\"data row5 col5\" >22.05%</td>\n",
       "      <td id=\"T_63846_row5_col6\" class=\"data row5 col6\" >59.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63846_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_63846_row6_col0\" class=\"data row6 col0\" >MCI</td>\n",
       "      <td id=\"T_63846_row6_col1\" class=\"data row6 col1\" >LIV</td>\n",
       "      <td id=\"T_63846_row6_col2\" class=\"data row6 col2\" >Apr01</td>\n",
       "      <td id=\"T_63846_row6_col3\" class=\"data row6 col3\" >1 - 0</td>\n",
       "      <td id=\"T_63846_row6_col4\" class=\"data row6 col4\" >52.65%</td>\n",
       "      <td id=\"T_63846_row6_col5\" class=\"data row6 col5\" >24.61%</td>\n",
       "      <td id=\"T_63846_row6_col6\" class=\"data row6 col6\" >22.74%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63846_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_63846_row7_col0\" class=\"data row7 col0\" >NEW</td>\n",
       "      <td id=\"T_63846_row7_col1\" class=\"data row7 col1\" >MUN</td>\n",
       "      <td id=\"T_63846_row7_col2\" class=\"data row7 col2\" >Apr02</td>\n",
       "      <td id=\"T_63846_row7_col3\" class=\"data row7 col3\" >1 - 1</td>\n",
       "      <td id=\"T_63846_row7_col4\" class=\"data row7 col4\" >36.28%</td>\n",
       "      <td id=\"T_63846_row7_col5\" class=\"data row7 col5\" >26.88%</td>\n",
       "      <td id=\"T_63846_row7_col6\" class=\"data row7 col6\" >36.83%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_63846_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_63846_row8_col0\" class=\"data row8 col0\" >WHU</td>\n",
       "      <td id=\"T_63846_row8_col1\" class=\"data row8 col1\" >SOU</td>\n",
       "      <td id=\"T_63846_row8_col2\" class=\"data row8 col2\" >Apr02</td>\n",
       "      <td id=\"T_63846_row8_col3\" class=\"data row8 col3\" >1 - 1</td>\n",
       "      <td id=\"T_63846_row8_col4\" class=\"data row8 col4\" >51.34%</td>\n",
       "      <td id=\"T_63846_row8_col5\" class=\"data row8 col5\" >24.90%</td>\n",
       "      <td id=\"T_63846_row8_col6\" class=\"data row8 col6\" >23.76%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2bc80a186a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<style type=\"text/css\">\n",
      "#T_63846_row0_col4 {\n",
      "  background-color: #023858;\n",
      "  color: #f1f1f1;\n",
      "}\n",
      "#T_63846_row0_col5 {\n",
      "  background-color: #ede7f2;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row0_col6 {\n",
      "  background-color: #fff7fb;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row1_col4 {\n",
      "  background-color: #348ebf;\n",
      "  color: #f1f1f1;\n",
      "}\n",
      "#T_63846_row1_col5 {\n",
      "  background-color: #d2d3e7;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row1_col6 {\n",
      "  background-color: #d0d1e6;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row2_col4 {\n",
      "  background-color: #dddbec;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row2_col5 {\n",
      "  background-color: #d6d6e9;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row2_col6 {\n",
      "  background-color: #167bb6;\n",
      "  color: #f1f1f1;\n",
      "}\n",
      "#T_63846_row3_col4 {\n",
      "  background-color: #3f93c2;\n",
      "  color: #f1f1f1;\n",
      "}\n",
      "#T_63846_row3_col5 {\n",
      "  background-color: #d1d2e6;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row3_col6 {\n",
      "  background-color: #cacee5;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row4_col4 {\n",
      "  background-color: #a1bbda;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row4_col5, #T_63846_row7_col5 {\n",
      "  background-color: #ced0e6;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row4_col6 {\n",
      "  background-color: #7eadd1;\n",
      "  color: #f1f1f1;\n",
      "}\n",
      "#T_63846_row5_col4 {\n",
      "  background-color: #f0eaf4;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row5_col5 {\n",
      "  background-color: #e2dfee;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row5_col6 {\n",
      "  background-color: #045e94;\n",
      "  color: #f1f1f1;\n",
      "}\n",
      "#T_63846_row6_col4 {\n",
      "  background-color: #0f76b3;\n",
      "  color: #f1f1f1;\n",
      "}\n",
      "#T_63846_row6_col5 {\n",
      "  background-color: #d8d7e9;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row6_col6 {\n",
      "  background-color: #e0dded;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row7_col4 {\n",
      "  background-color: #91b5d6;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row7_col6 {\n",
      "  background-color: #8eb3d5;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row8_col4 {\n",
      "  background-color: #187cb6;\n",
      "  color: #f1f1f1;\n",
      "}\n",
      "#T_63846_row8_col5 {\n",
      "  background-color: #d7d6e9;\n",
      "  color: #000000;\n",
      "}\n",
      "#T_63846_row8_col6 {\n",
      "  background-color: #dbdaeb;\n",
      "  color: #000000;\n",
      "}\n",
      "</style>\n",
      "<table id=\"T_63846\">\n",
      "  <thead>\n",
      "    <tr>\n",
      "      <th class=\"blank level0\" >&nbsp;</th>\n",
      "      <th id=\"T_63846_level0_col0\" class=\"col_heading level0 col0\" >Home</th>\n",
      "      <th id=\"T_63846_level0_col1\" class=\"col_heading level0 col1\" >Away</th>\n",
      "      <th id=\"T_63846_level0_col2\" class=\"col_heading level0 col2\" >Date</th>\n",
      "      <th id=\"T_63846_level0_col3\" class=\"col_heading level0 col3\" >Most likely score</th>\n",
      "      <th id=\"T_63846_level0_col4\" class=\"col_heading level0 col4\" >1</th>\n",
      "      <th id=\"T_63846_level0_col5\" class=\"col_heading level0 col5\" >X</th>\n",
      "      <th id=\"T_63846_level0_col6\" class=\"col_heading level0 col6\" >2</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th id=\"T_63846_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
      "      <td id=\"T_63846_row0_col0\" class=\"data row0 col0\" >ARS</td>\n",
      "      <td id=\"T_63846_row0_col1\" class=\"data row0 col1\" >LEE</td>\n",
      "      <td id=\"T_63846_row0_col2\" class=\"data row0 col2\" >Apr01</td>\n",
      "      <td id=\"T_63846_row0_col3\" class=\"data row0 col3\" >2 - 0</td>\n",
      "      <td id=\"T_63846_row0_col4\" class=\"data row0 col4\" >67.89%</td>\n",
      "      <td id=\"T_63846_row0_col5\" class=\"data row0 col5\" >19.43%</td>\n",
      "      <td id=\"T_63846_row0_col6\" class=\"data row0 col6\" >12.68%</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th id=\"T_63846_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
      "      <td id=\"T_63846_row1_col0\" class=\"data row1 col0\" >BHA</td>\n",
      "      <td id=\"T_63846_row1_col1\" class=\"data row1 col1\" >BRE</td>\n",
      "      <td id=\"T_63846_row1_col2\" class=\"data row1 col2\" >Apr01</td>\n",
      "      <td id=\"T_63846_row1_col3\" class=\"data row1 col3\" >1 - 1</td>\n",
      "      <td id=\"T_63846_row1_col4\" class=\"data row1 col4\" >47.49%</td>\n",
      "      <td id=\"T_63846_row1_col5\" class=\"data row1 col5\" >25.99%</td>\n",
      "      <td id=\"T_63846_row1_col6\" class=\"data row1 col6\" >26.52%</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th id=\"T_63846_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
      "      <td id=\"T_63846_row2_col0\" class=\"data row2 col0\" >BOU</td>\n",
      "      <td id=\"T_63846_row2_col1\" class=\"data row2 col1\" >FUL</td>\n",
      "      <td id=\"T_63846_row2_col2\" class=\"data row2 col2\" >Apr01</td>\n",
      "      <td id=\"T_63846_row2_col3\" class=\"data row2 col3\" >0 - 1</td>\n",
      "      <td id=\"T_63846_row2_col4\" class=\"data row2 col4\" >23.36%</td>\n",
      "      <td id=\"T_63846_row2_col5\" class=\"data row2 col5\" >24.99%</td>\n",
      "      <td id=\"T_63846_row2_col6\" class=\"data row2 col6\" >51.65%</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th id=\"T_63846_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
      "      <td id=\"T_63846_row3_col0\" class=\"data row3 col0\" >CHE</td>\n",
      "      <td id=\"T_63846_row3_col1\" class=\"data row3 col1\" >AVL</td>\n",
      "      <td id=\"T_63846_row3_col2\" class=\"data row3 col2\" >Apr01</td>\n",
      "      <td id=\"T_63846_row3_col3\" class=\"data row3 col3\" >1 - 1</td>\n",
      "      <td id=\"T_63846_row3_col4\" class=\"data row3 col4\" >46.31%</td>\n",
      "      <td id=\"T_63846_row3_col5\" class=\"data row3 col5\" >26.27%</td>\n",
      "      <td id=\"T_63846_row3_col6\" class=\"data row3 col6\" >27.43%</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th id=\"T_63846_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
      "      <td id=\"T_63846_row4_col0\" class=\"data row4 col0\" >CRY</td>\n",
      "      <td id=\"T_63846_row4_col1\" class=\"data row4 col1\" >LEI</td>\n",
      "      <td id=\"T_63846_row4_col2\" class=\"data row4 col2\" >Apr01</td>\n",
      "      <td id=\"T_63846_row4_col3\" class=\"data row4 col3\" >1 - 1</td>\n",
      "      <td id=\"T_63846_row4_col4\" class=\"data row4 col4\" >34.22%</td>\n",
      "      <td id=\"T_63846_row4_col5\" class=\"data row4 col5\" >26.81%</td>\n",
      "      <td id=\"T_63846_row4_col6\" class=\"data row4 col6\" >38.98%</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th id=\"T_63846_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
      "      <td id=\"T_63846_row5_col0\" class=\"data row5 col0\" >EVE</td>\n",
      "      <td id=\"T_63846_row5_col1\" class=\"data row5 col1\" >TOT</td>\n",
      "      <td id=\"T_63846_row5_col2\" class=\"data row5 col2\" >Apr03</td>\n",
      "      <td id=\"T_63846_row5_col3\" class=\"data row5 col3\" >0 - 1</td>\n",
      "      <td id=\"T_63846_row5_col4\" class=\"data row5 col4\" >18.45%</td>\n",
      "      <td id=\"T_63846_row5_col5\" class=\"data row5 col5\" >22.05%</td>\n",
      "      <td id=\"T_63846_row5_col6\" class=\"data row5 col6\" >59.50%</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th id=\"T_63846_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
      "      <td id=\"T_63846_row6_col0\" class=\"data row6 col0\" >MCI</td>\n",
      "      <td id=\"T_63846_row6_col1\" class=\"data row6 col1\" >LIV</td>\n",
      "      <td id=\"T_63846_row6_col2\" class=\"data row6 col2\" >Apr01</td>\n",
      "      <td id=\"T_63846_row6_col3\" class=\"data row6 col3\" >1 - 0</td>\n",
      "      <td id=\"T_63846_row6_col4\" class=\"data row6 col4\" >52.65%</td>\n",
      "      <td id=\"T_63846_row6_col5\" class=\"data row6 col5\" >24.61%</td>\n",
      "      <td id=\"T_63846_row6_col6\" class=\"data row6 col6\" >22.74%</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th id=\"T_63846_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
      "      <td id=\"T_63846_row7_col0\" class=\"data row7 col0\" >NEW</td>\n",
      "      <td id=\"T_63846_row7_col1\" class=\"data row7 col1\" >MUN</td>\n",
      "      <td id=\"T_63846_row7_col2\" class=\"data row7 col2\" >Apr02</td>\n",
      "      <td id=\"T_63846_row7_col3\" class=\"data row7 col3\" >1 - 1</td>\n",
      "      <td id=\"T_63846_row7_col4\" class=\"data row7 col4\" >36.28%</td>\n",
      "      <td id=\"T_63846_row7_col5\" class=\"data row7 col5\" >26.88%</td>\n",
      "      <td id=\"T_63846_row7_col6\" class=\"data row7 col6\" >36.83%</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th id=\"T_63846_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
      "      <td id=\"T_63846_row8_col0\" class=\"data row8 col0\" >WHU</td>\n",
      "      <td id=\"T_63846_row8_col1\" class=\"data row8 col1\" >SOU</td>\n",
      "      <td id=\"T_63846_row8_col2\" class=\"data row8 col2\" >Apr02</td>\n",
      "      <td id=\"T_63846_row8_col3\" class=\"data row8 col3\" >1 - 1</td>\n",
      "      <td id=\"T_63846_row8_col4\" class=\"data row8 col4\" >51.34%</td>\n",
      "      <td id=\"T_63846_row8_col5\" class=\"data row8 col5\" >24.90%</td>\n",
      "      <td id=\"T_63846_row8_col6\" class=\"data row8 col6\" >23.76%</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(final)\n",
    "\n",
    "print(final.to_html(index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>Away</th>\n",
       "      <th>Date</th>\n",
       "      <th>Most likely score</th>\n",
       "      <th>1</th>\n",
       "      <th>X</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARS</td>\n",
       "      <td>LEE</td>\n",
       "      <td>Apr01</td>\n",
       "      <td>2 - 0</td>\n",
       "      <td>0.678936</td>\n",
       "      <td>0.194268</td>\n",
       "      <td>0.126796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BHA</td>\n",
       "      <td>BRE</td>\n",
       "      <td>Apr01</td>\n",
       "      <td>1 - 1</td>\n",
       "      <td>0.474950</td>\n",
       "      <td>0.259879</td>\n",
       "      <td>0.265172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BOU</td>\n",
       "      <td>FUL</td>\n",
       "      <td>Apr01</td>\n",
       "      <td>0 - 1</td>\n",
       "      <td>0.233567</td>\n",
       "      <td>0.249922</td>\n",
       "      <td>0.516511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHE</td>\n",
       "      <td>AVL</td>\n",
       "      <td>Apr01</td>\n",
       "      <td>1 - 1</td>\n",
       "      <td>0.463058</td>\n",
       "      <td>0.262684</td>\n",
       "      <td>0.274258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CRY</td>\n",
       "      <td>LEI</td>\n",
       "      <td>Apr01</td>\n",
       "      <td>1 - 1</td>\n",
       "      <td>0.342169</td>\n",
       "      <td>0.268056</td>\n",
       "      <td>0.389776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EVE</td>\n",
       "      <td>TOT</td>\n",
       "      <td>Apr03</td>\n",
       "      <td>0 - 1</td>\n",
       "      <td>0.184477</td>\n",
       "      <td>0.220490</td>\n",
       "      <td>0.595033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MCI</td>\n",
       "      <td>LIV</td>\n",
       "      <td>Apr01</td>\n",
       "      <td>1 - 0</td>\n",
       "      <td>0.526458</td>\n",
       "      <td>0.246149</td>\n",
       "      <td>0.227393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NEW</td>\n",
       "      <td>MUN</td>\n",
       "      <td>Apr02</td>\n",
       "      <td>1 - 1</td>\n",
       "      <td>0.362848</td>\n",
       "      <td>0.268818</td>\n",
       "      <td>0.368335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WHU</td>\n",
       "      <td>SOU</td>\n",
       "      <td>Apr02</td>\n",
       "      <td>1 - 1</td>\n",
       "      <td>0.513387</td>\n",
       "      <td>0.248986</td>\n",
       "      <td>0.237627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Home Away   Date Most likely score         1         X         2\n",
       "0  ARS  LEE  Apr01             2 - 0  0.678936  0.194268  0.126796\n",
       "1  BHA  BRE  Apr01             1 - 1  0.474950  0.259879  0.265172\n",
       "2  BOU  FUL  Apr01             0 - 1  0.233567  0.249922  0.516511\n",
       "3  CHE  AVL  Apr01             1 - 1  0.463058  0.262684  0.274258\n",
       "4  CRY  LEI  Apr01             1 - 1  0.342169  0.268056  0.389776\n",
       "5  EVE  TOT  Apr03             0 - 1  0.184477  0.220490  0.595033\n",
       "6  MCI  LIV  Apr01             1 - 0  0.526458  0.246149  0.227393\n",
       "7  NEW  MUN  Apr02             1 - 1  0.362848  0.268818  0.368335\n",
       "8  WHU  SOU  Apr02             1 - 1  0.513387  0.248986  0.237627"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.rename(columns={\"Club_x\": \"Home\", \"Opp_x\": \"Away\",\"win_prob\": \"1\", \"draw_prob\":\"X\",\"loss_prob\":\"2\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
